# microrad

Micrograd — Clean Educational Reimplementation

A minimal, from-scratch automatic differentiation engine inspired by Andrej Karpathy’s micrograd, implemented with clarity and pedagogical intent.

This project demonstrates how reverse-mode autodiff (backpropagation) works under the hood by building a tiny scalar-based computation graph engine without relying on PyTorch, TensorFlow, or JAX.

## Features

- Scalar automatic differentiation

- Dynamic computation graph

- Reverse-mode backpropagation

- Supported operations:

-- +, -, *, /, **

-- ReLU, Sigmoid

- No external ML libraries
